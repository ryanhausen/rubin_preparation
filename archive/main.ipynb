{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14c076e-534f-43e2-8213-24ba735d598b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Imports\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from astropy import nddata\n",
    "from astropy.io import fits\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from jax.random import PRNGKey # Need to use a seed to start jax's random number generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310a6bb7-e720-4734-8a4e-d27ea8ae7d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "# Acquiring Image, PSF, Variance ------------#\n",
    "##############################################\n",
    "\n",
    "# Opening Image\n",
    "img = 'rsb_fits_images/test1.fits'\n",
    "hdu = fits.open(img)\n",
    "\n",
    "uncertainty_data = hdu[3].data    # variance\n",
    "img_data = hdu[1].data            # image\n",
    "\n",
    "psfex_hdu_data_index = 10\n",
    "psfex_hdu_info_index = 9 \n",
    "\n",
    "# Acquiring Workable PSF\n",
    "psfex_info = hdu[psfex_hdu_info_index]\n",
    "psfex_data = hdu[psfex_hdu_data_index]\n",
    "\n",
    "pixstep = psfex_info.data._pixstep[0]  # Image pixel per PSF pixel\n",
    "size = psfex_data.data[\"_size\"]  # size of PSF  (nx, ny, n_basis_vectors)\n",
    "comp = psfex_data.data[\"_comp\"]  # PSF basis components\n",
    "coeff = psfex_data.data[\"coeff\"]  # Coefficients modifying each basis vector\n",
    "psf_basis_image = comp[0].reshape(*size[0][::-1])\n",
    "psf_image = psf_basis_image * psfex_data.data[\"basis\"][0, :, np.newaxis, np.newaxis]\n",
    "psf_image = psf_image.sum(0)\n",
    "psf_image /= psf_image.sum() * pixstep**2\n",
    "\n",
    "# Plotting PSF\n",
    "# plt.imshow(psf_image, cmap='gray', interpolation='none',vmin=-0.0001, vmax=0.0001)\n",
    "# plt.xlabel('X-axis')\n",
    "# plt.ylabel('Y-axis')\n",
    "# plt.show()\n",
    "\n",
    "# PLotting Retrieved FITS Image\n",
    "# plt.imshow(img_data.data, vmin=0, vmax=0.3,origin=\"lower\", cmap=\"gray\")\n",
    "# plt.title('Original Image Array')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792ff5f0-6104-45b4-babc-795c1d811259",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_mask(image_shape):\n",
    "    import jax.numpy as jnp\n",
    "    return jnp.array(np.zeros(image_shape))\n",
    "\n",
    "def gen_psf(image_shape):\n",
    "    psf = np.zeros(image_shape)\n",
    "    center = (image_shape[0] // 2, image_shape[1] // 2)\n",
    "    psf[center] = 1\n",
    "    sigma = 2  \n",
    "    psf = gaussian_filter(psf, sigma=sigma)\n",
    "    psf /= psf.sum()\n",
    "    return psf\n",
    "\n",
    "def resize_psf(psf_image, new_shape):\n",
    "    import cv2\n",
    "    resized_psf = cv2.resize(psf_image, new_shape, interpolation=cv2.INTER_AREA)\n",
    "    resized_psf /= np.sum(resized_psf)    \n",
    "    return resized_psf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4a4c4e-7479-4060-bc29-1f407d559303",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "# Identifying Sources & Creating Cutouts ----#\n",
    "##############################################\n",
    "\n",
    "# Defining Threshold in Relation to Background RMS (how bright a source must be)\n",
    "from photutils.background import Background2D, MedianBackground\n",
    "bkg_estimator = MedianBackground()\n",
    "bkg = Background2D(img_data, (50, 50), filter_size=(3, 3), bkg_estimator=bkg_estimator)\n",
    "threshold = 1.9 * bkg.background_rms \n",
    "\n",
    "\n",
    "from astropy.convolution import convolve\n",
    "from photutils.segmentation import make_2dgaussian_kernel\n",
    "kernel = make_2dgaussian_kernel(3.0, size=5)  # FWHM = 3.0\n",
    "convolved_data = convolve(img_data, kernel)\n",
    "\n",
    "\n",
    "# Detecting Sources --> Segmentation Map\n",
    "from photutils.segmentation import detect_sources\n",
    "from matplotlib.colors import LogNorm\n",
    "segment_map = detect_sources(convolved_data, threshold, npixels=10)\n",
    "# display segment map\n",
    "# plt.imshow(segment_map,cmap=\"gray\", origin=\"lower\", vmin=0, vmax=500)\n",
    "segment_map.remove_border_labels(10, partial_overlap=False, relabel=True)\n",
    "\n",
    "print(len(segment_map.labels))\n",
    "\n",
    "bbox = segment_map.bbox\n",
    "assert(len(segment_map.labels) == len(bbox)) # should be as many labels as bboxes\n",
    "\n",
    "labels = segment_map.labels\n",
    "# Creating Same-Dimension Cutouts of Image, Mask, and Variance\n",
    "cutouts = []\n",
    "for i in range(len(shortened_bbox)):\n",
    "    y_center, x_center = shortened_bbox[i].center\n",
    "    x_len,y_len = shortened_bbox[i].shape\n",
    "    min_length = 12 #22\n",
    "    if (x_len> 10 and y_len > 10 and x_len < 40 and y_len < 40):\n",
    "        length = max([x_len, y_len, min_length]) * 1.25\n",
    "        my_cutout = nddata.Cutout2D(img_data, (x_center,y_center), int(length))\n",
    "        cutout_mask = gen_mask(my_cutout.shape)\n",
    "        # generated_psf = gen_psf(my_cutout.shape)\n",
    "        # normalized_psf = generated_psf/np.sum(generated_psf)\n",
    "        actual_psf = resize_psf(psf_image, my_cutout.shape)\n",
    "        cutout_var = nddata.Cutout2D(uncertainty_data, (x_center,y_center), int(length))\n",
    "        package = [my_cutout, cutout_mask, cutout_var, actual_psf,labels[i]] #normalized_psf,actual_psf]\n",
    "        cutouts.append(package)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ae3198c-c89f-4e18-93a3-5452233d3633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1905\n"
     ]
    }
   ],
   "source": [
    "print(len(cutouts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f53e07f-0b04-4459-9eaf-a13718d69a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "# Fitting a Sersic Profile ------------------#\n",
    "##############################################\n",
    "\n",
    "from pysersic import FitSingle\n",
    "from pysersic.loss import student_t_loss\n",
    "from pysersic import results\n",
    "from pysersic.priors import autoprior\n",
    "from pysersic.priors import SourceProperties\n",
    "from pysersic import check_input_data\n",
    "from pysersic import FitSingle\n",
    "from pysersic.loss import gaussian_loss\n",
    "from pysersic.results import plot_residual\n",
    "\n",
    "# future --> test different loss functions --> student-t-loss, gaussian\n",
    "\n",
    "# now we begin labelling the segmentation map\n",
    "labelled_seg = np.zeros((segment_map.shape[0],segment_map.shape[1],3))\n",
    "\n",
    "for i in range(len(cutouts)):\n",
    "    \n",
    "    im,mask,sig,psf,label = cutouts[i] # image, mask, variance, psf\n",
    "    if (im.shape[0] != psf.shape[0] or im.shape[1] != psf.shape[1]):\n",
    "        print('weird size mismatch for some reason')\n",
    "        # fig, axes = plt.subplots(1,2)\n",
    "        # axes[0].imshow(im.data,cmap=\"gray\")\n",
    "        # axes[1].imshow(psf,cmap='gray')\n",
    "        \n",
    "    else:\n",
    "        # Plotting Cutout & PSF\n",
    "        # fig, axes = plt.subplots(1, 1, figsize=(12, 12))\n",
    "        #     axes[0].imshow(im.data, cmap='gray', origin=\"lower\")\n",
    "        #     axes[0].set_title('Cutout')\n",
    "        #     axes[0].axis('off')\n",
    "        #     plt.show()\n",
    "\n",
    "        # Verify Components are Usable\n",
    "        check_input_data(im.data, sig.data, psf, mask)\n",
    "\n",
    "        # Prior Estimation of Parameters\n",
    "        props = SourceProperties(im.data,mask=mask) \n",
    "        prior = props.generate_prior('sersic',sky_type='none')\n",
    "\n",
    "        # Fit\n",
    "        fitter = FitSingle(data=im.data,rms=sig.data, psf=psf, prior=prior, mask=mask, loss_func=gaussian_loss) \n",
    "        map_params = fitter.find_MAP(rkey = PRNGKey(1000));               # contains dictionary of Sersic values\n",
    "        fig, ax = plot_residual(im.data,map_params['model'],mask=mask,vmin=-1,vmax=1);\n",
    "        fig.suptitle(\"Analysis of Fit\")\n",
    "\n",
    "        ##############################################\n",
    "        # Testing Fit -------------------------------#\n",
    "        # (does it belong in training dataset)\n",
    "        ##############################################\n",
    "        image = im.data\n",
    "        model = map_params['model']\n",
    "        assert(image.shape == model.shape)\n",
    "\n",
    "        # Chi-squared Statistic ----------------------------------------------------------------#\n",
    "        # (evaluating whether the difference in Image and Model is systematic or due to noise)\n",
    "\n",
    "        from scipy.stats import chi2\n",
    "        chi_square = np.sum((image*2.2 - model) ** 2 / (model))\n",
    "        df = image.size-1                                                 # number of categories - 1\n",
    "        p_value = chi2.sf(chi_square, df)\n",
    "        \n",
    "        ##############################################\n",
    "        # Labelling the Segmap ----------------------#\n",
    "        ##############################################\n",
    "        # segmap id (pixel value??)\n",
    "        # n \n",
    "        # p-value from fit\n",
    "        \n",
    "        n = map_params['n']\n",
    "        # print(im.xmin_original, im.ymin_original,im.xmax_original,im.ymax_original)\n",
    "\n",
    "        for xpos in range(im.xmin_original, im.xmax_original,1):\n",
    "            for ypos in range(im.ymin_original, im.ymax_original,1):\n",
    "                labelled_seg[xpos][ypos] = [label, n, p_value]\n",
    "            \n",
    "\n",
    "                \n",
    "#         noise_threshold = np.mean(sig.data)                               # average of the variance\n",
    "#         image_1D = image.flatten()\n",
    "#         model_1D = model.flatten()\n",
    "#         difference_1D = image_1D - model_1D\n",
    "#         l1 = np.sum(np.abs(difference_1D))\n",
    "#         l1_normalized = l1/(image_1D.size)\n",
    "#         print(f\"L1 norm: {l1_normalized}\")\n",
    "        \n",
    "#         if (p_value < noise_threshold):\n",
    "#             print(f\"This model is not good.{p_value}\")\n",
    "#             # bad_list.append([image,model,p_value,l1_normalized])\n",
    "#         else:\n",
    "#             print(f\"This model is a good fit.{p_value}\")\n",
    "#             good_list.append({\n",
    "#                 'image': im,\n",
    "#                 'r_eff': map_params['r_eff'],\n",
    "#             \n",
    "# 'n': map_params['n']\n",
    "#             })\n",
    "            # good_list.append([im.data,model,p_value,l1_normalized])\n",
    "\n",
    "        # L1-Norm Statistic/Manhatten Distance (1) ---------------------------------------------#\n",
    "        # (sum of absolute value of (actual pixel value - model pixel value) for all pixels)\n",
    "\n",
    "        # what threshold should the l1_normalized metric meet?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5a26ce-91b3-4b10-aedc-7ee51350a4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(labelled_seg)):\n",
    "    for j in range(len(labelled_seg[0])):\n",
    "        for k in range(3):\n",
    "            if (labelled_seg[i][j][k] != 0):\n",
    "                print(labelled_seg[i][j][k], end=\"\")\n",
    "    # print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78caa584-0ea1-4355-808c-b62fc4d54c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in bad_list:\n",
    "#     fig, axes = plt.subplots(1,3)\n",
    "#     axes[0].imshow(i[0],cmap=\"gray\", vmin=0, vmax=0.5)\n",
    "#     axes[1].imshow(i[1],cmap=\"gray\", vmin=0, vmax=0.5)\n",
    "#     axes[2].imshow(i[0]-i[1],cmap=\"gray\",vmin=0, vmax=0.1)\n",
    "\n",
    "#     fig.suptitle(f\"Image (LEFT)       Model (MIDDLE)         Difference (RIGHT)     p-val:{i[2]}    l1_norm:{i[3]}\")\n",
    "# for i in good_list:\n",
    "#     fig, axes = plt.subplots(1,3)\n",
    "#     axes[0].imshow(i[0],cmap=\"gray\", vmin=0, vmax=0.5)\n",
    "#     axes[1].imshow(i[1],cmap=\"gray\", vmin=0, vmax=0.5)\n",
    "#     axes[2].imshow(i[0]-i[1],cmap=\"gray\",vmin=0, vmax=0.1)\n",
    "\n",
    "#     fig.suptitle(f\"Image (LEFT)       Model (MIDDLE)         Difference (RIGHT)     p-val:{i[2]}    l1_norm:{i[3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "1aee8772-1b48-4ee0-9ed5-79b864b6d345",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSources:\\n1) https://medium.com/swlh/different-types-of-distances-used-in-machine-learning-ec7087616442#:~:text=L1%20Norm%3A,the%20components%20of%20the%20vectors.\\n\\n'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Sources:\n",
    "1) https://medium.com/swlh/different-types-of-distances-used-in-machine-learning-ec7087616442#:~:text=L1%20Norm%3A,the%20components%20of%20the%20vectors.\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 via cpuq",
   "language": "python",
   "name": "cpuq-py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
